[
  {
    "path": "posts/2022-03-07-tidymodels-rsample/",
    "title": "Resampling with rsample",
    "description": "A Gentle Introduction to tidymodels::rsample library and common Resampling Methods.",
    "author": [],
    "date": "2022-03-07",
    "categories": [
      "Tutorials",
      "R"
    ],
    "contents": "\n\n\n\nR is the introductory programming language in my journey of learning data science, and I love using it for exploratory analysis, to plot graphs, or clean data. This is mainly due to the variety of convenient tools in the tidyverse packages. However, Python is my go-to for anything related to building predictive models and machine learning. During my last internship, R was the main program that we used for data related projects and that’s where I started to pick up R for machine learning.\nRecently I’ve stumbled upon the tidymodels library, which is a collection of packages for modeling and machine learning…the tidy way. I wanted to write a series of posts sharing what I learned from each library by showing examples of use cases and code snippets, along with explaining some technical terms along with way. We will start off with the samplr package.\nHere’s what we’ll focus on :\nThe rsample library\nIntro to rsample\nInstallation\n\nOverview on resampling methods\nWhat is resampling?\nWhy does it matter?\n\nThe dataset\nrset vs. rsplit objects\nSplit training and testing sets\ninitial_split()\ninitial_time_split()\ntraining(), testing()\n\nBootstrapping\nbootstraps()\n\nCross Validation\nV-fold Cross-Validation (K-fold Cross-Validation) - vfold_cv()\n\nThe rsample library\nIntro to rsample\nThe rsample package contains functions that help split the data into different sample sets for analysis using resampling methods. Before we talk about resampling methods, let’s start with installing the package.\nInstallation\nYou can get the samplr package by installing the tidymodels meta-library:\n\n\n# Install from CRAN\ninstall.packages(\"tidymodels\")\n\n\n\n\n\n# Install from GitHub\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_dev(\"tidymodels\")\n\n\n\nOr just the rsample package:\n\n\n# Install from CRAN\ninstall.packages(\"rsample\")\n\n\n\n\n\n# Install from GitHub\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_dev(\"rsample\")\n\n\n\nThen load in the libraries that we’ll be using:\n\n\n# Load in the libraries\nlibrary(rsample)\nlibrary(tidyverse)\n\n\n\nNow that we’ve installed and loaded the packages, let’s go over some terms related to resampling.\nOverview on resampling methods\nWhat is resampling?\nTo simply put, statistical resampling is drawing a sample set from the available data, measure the statistics(mean, standard deviation, etc.) on the sample then repeating the process x amount of times. This repeated process tells us a better estimation about the statistics of the population. It becomes useful when we have a small sample data.\nIn machine learning, we use resampling to test the performance of a model. For example, Bootstrapping is a method of resampling where we randomly draw the same size of sample from the original data with replacement (observation can occur more than once in the sample set). This becomes the training set for the model, and the remaining set that wasn’t drawn would be used for testing the model. In contrast, Cross-Validation is when the data is drawn randomly without replacement. We will look at different Cross-Validations with examples later in the post.\nWhy does it matter?\nDue to the nature of randomness, a model could be trained on a set of data with bias or a certain skewness that doesn’t represent the original data set. Or the model could be excellent at predicting on one specific testing set sample. This could lead to false conclusions about the model performance. To avoid making invalid conclusions, it’s important to take time to validate your sample.\nThe downside of resampling could be how computationally expensive it gets with the amount of resamples and iterations of the analysis.\nI would recommend further study on resampling methods as I haven’t gotten into much depth. Check out this article on Resampling Methods.\nThe dataset\nWe’ll use the Bank Marketing dataset – a direct marketing campaign of making phone calls to clients for subscription to a term deposit. Here’s what our data look like:\n\n\n# Read in dataset\nbank_df <- \n  read.csv('bank-full.csv', sep=';')\n\n\n\nBank marketing data previewThere are 45,211 rows and 17 columns in the dataset. Let’s check for any missing values:\n\n\n# Total missing values\nbank_df %>% \n  is.na() %>% \n  sum()\n\n\n[1] 0\n\nThere are 0 missing values. Let’s check the balance of the target variable column to see what percentage of the clients subscribed to the term deposit using the column “y”:\n\n\n# Check target variable proportion\nbank_df %>% \n  count(y) %>% \n  mutate(prop = n/sum(n))\n\n\n    y     n      prop\n1  no 39922 0.8830152\n2 yes  5289 0.1169848\n\nAbout 11% of the phone calls had a successful outcome of the client agreeing on a subscription. Luckily the data seems relatively clean and doesn’t include any missing data. We will work on some exploratory work on a different article. For now, let’s get familiar with some rsample functions and use cases.\nrset vs. rsplit objects\nThe rsample package uses rset and rsplit class objects that are derived from different methods of splitting data sets. rset objects are a tibble object that contains a collection of resamples. Each resamples are stored as an rsplit object in the splits column of the rset object.\n ADD DRAWING\nSplit training and testing sets\nLet’s create our training and testing data sets using the inital_split() function. The prop argument defines the split proportion which in our case training set will contain 80% of the original data to train the model, while remaining 20% set will be used to check the model performance.\n\n\n\nOnce we define our split, we use the training(), testing() functions to extract the data and assign to variables.\n\n\n\nNotice the initial_split() function classifies the splits as Analysis, Assessment. This is the same as the train and test split but since we already have training() and testing() functions, it uses Analysis and Assessment to avoid confusion.\n\n<Analysis/Assess/Total>\n<36168/9043/45211>\n\nBootstrapping with rsample\nAs we mentioned earlier that bootstrapping samples the data set with replacements. The analysis set may contain data with repeated samples, and the assessment set included data that weren’t part of the analysis set. It uses the function bootstraps(), which returns a tibble with the amount of samples defined in the times argument. Below we have a rset object containing 4 rsplit objects with [# of analysis samples/# of assessment samples]. Notice that the analysis sample size is the same as the original data size, and the assessment sample sizes vary.\n\n\n\n\n\n# Bootstrap sampling with 4 sample sets\nbootstrap_samples <- bootstraps(bank_df, times=4)\nbootstrap_samples\n\n\n# Bootstrap sampling \n# A tibble: 4 × 2\n  splits                id        \n  <list>                <chr>     \n1 <split [45211/16660]> Bootstrap1\n2 <split [45211/16651]> Bootstrap2\n3 <split [45211/16665]> Bootstrap3\n4 <split [45211/16646]> Bootstrap4\n\nIn order to extract the analysis and assessment samples we would define it the data parameter:\n\n\n# select the bootstrap sample by index\n# in this case we'll select the first resample\nfirst_resample <- bootstrap_samples$splits[[1]]\n\n# extract the analysis\nfirst_resample_analysis <- as.data.frame(first_resample, data='analysis')\n# extract the assessment\nfirst_resample_assessment <- as.data.frame(first_resample, data='assessment')\n\n\n\nCross-Validations with rsample\nV-fold Cross-Validation (K-fold Cross-Validation)\nV-fold (also known as K-fold) Cross-Validation is when we split the data into “v” number of equal groups. For example, when v=10, then we have a 10-fold cross-validation. If we have a 10-fold cross-validation, the data is split into 10 almost equal sets. We pick the first group for evaluating our model and use the remaining 9 groups to train the model. We repeat the process until each group gets to be used as a test set. This resampling method is used widely in machine learning due to how well it could measure a model’s performance by reducing the bias in the data. It’s also a great way to tune the hyperparameters of the model.\nLets try it out using the vfold_cv() function:\n\n\nvfold <- vfold_cv(bank_df, v = 10)\nvfold\n\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits               id    \n   <list>               <chr> \n 1 <split [40689/4522]> Fold01\n 2 <split [40690/4521]> Fold02\n 3 <split [40690/4521]> Fold03\n 4 <split [40690/4521]> Fold04\n 5 <split [40690/4521]> Fold05\n 6 <split [40690/4521]> Fold06\n 7 <split [40690/4521]> Fold07\n 8 <split [40690/4521]> Fold08\n 9 <split [40690/4521]> Fold09\n10 <split [40690/4521]> Fold10\n\nSummary\nWe went over some resampling methods and apply them by using rsample library. In the following series, we’ll go over some more tidymodels libraries and also see how these functions are applied in model development and assessing model performance.\n\n\n\n",
    "preview": "posts/2022-03-07-tidymodels-rsample/rsample.png",
    "last_modified": "2022-03-07T17:53:05-07:00",
    "input_file": {}
  }
]
